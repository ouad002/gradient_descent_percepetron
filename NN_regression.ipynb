{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d42d9ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <center>UP3, Optimization for machine learning: regression with a Neural Network from scratch </center>\n",
    "\n",
    "This notebook contains the questions of the practical session along with complementary guidelines and examples. The code is written in Python. The questions are in red.\n",
    "\n",
    "First import all given code. You are encouraged to have a look at forward_propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d072762-fdbf-4fac-85cf-af3c9ba1fbc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:12:38.955517Z",
     "start_time": "2025-11-24T15:12:37.986072Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List\n",
    "\n",
    "from gradient_descent import gradient_descent\n",
    "from optim_utilities import print_rec\n",
    "from test_functions import (\n",
    "    linear_function,\n",
    "    ackley,\n",
    "    sphere,\n",
    "    quadratic,\n",
    "    rosen,\n",
    "    L1norm,\n",
    "    sphereL1,\n",
    "    rastrigin,\n",
    "    michalewicz,\n",
    "    schwefel\n",
    ")\n",
    "\n",
    "from random_search import random_opt  # always useful to compare optim algos to a random search\n",
    "from restarted_gradient_descent import restarted_gradient_descent\n",
    "\n",
    "from gradient_descent_patched import gradient_descent_patched # nouveau import du descente gradient modifié\n",
    "from restarted_gradient_descent_patched import restarted_gradient_descent_patched # nouveau import du descente gradient\n",
    "# auto reload to reload functions imported that have been changed (cf. test_functions.sphereL1 for lbda)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52427bd6-0080-4d14-9820-df219bc26f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forward_propagation import (\n",
    "    forward_propagation, \n",
    "    create_weights, \n",
    "    vector_to_weights,\n",
    "    weights_to_vector)\n",
    "from activation_functions import (\n",
    "    relu,\n",
    "    sigmoid,\n",
    "    linear,\n",
    "    leaky_relu\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b875f8",
   "metadata": {},
   "source": [
    "### Data structure behind the forward propagation\n",
    "\n",
    "To start, let's have a look at the structure used to encode the network. Don't worry, more handy utility functions will be introduced soon to manipulate the network globally. \n",
    "\n",
    "The following network has 2 layers, the first going from 4 input components to the 3 internal neurons, the second going from the 3 internal neurons outputs to the 2 outputs. Don't forget the additional weight for the neurons biases.\n",
    "* `weight[l][i,j]` links entry `j` to output `i` in a layer `l`\n",
    "* the last column of the weights are the biases of the neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "inputs = np.array([[1,2,5,4],[1,0.2,0.15,0.024],[2,1.2,-1.01,-0.4]])\n",
    "# describe the neural net\n",
    "weights = [\n",
    "        np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1,-1],\n",
    "                [2,1,3,5,0],\n",
    "                [0.2,0.1,0.6,0.78,1]\n",
    "            ]\n",
    "        ),\n",
    "    np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1],\n",
    "                [2,1,3,5]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "activation = sigmoid\n",
    "# carry out forward propagation\n",
    "y=forward_propagation(inputs,weights,activation)\n",
    "# reporting\n",
    "print(f'This network has {inputs.shape[1]} inputs, {y.shape[1]} outputs and {inputs.shape[0]} data points')\n",
    "print('The predictions are:\\n',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb1fa2",
   "metadata": {},
   "source": [
    "### Create a data set \n",
    "The data set is made of points sampled randomly from a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829bf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_target(fun: Callable,\n",
    "                       n_features: int,\n",
    "                       n_obs: int,\n",
    "                       LB: List[float],\n",
    "                       UB: List[float]) -> dict:\n",
    "    \n",
    "    entry_data = np.random.uniform(low= LB,high=UB,\n",
    "                                   size=(n_obs, n_features))\n",
    "    target = np.apply_along_axis(fun, 1, entry_data)\n",
    "    \n",
    "    return {\"data\": entry_data, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34014047",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_function = linear_function\n",
    "n_features = 2\n",
    "n_obs = 10\n",
    "LB = [-5] * n_features\n",
    "UB = [5] * n_features\n",
    "simulated_data = simulate_data_target(fun = used_function,n_features = n_features,n_obs=n_obs,LB=LB,UB=UB)\n",
    "print('x,f(x) =\\n',np.concatenate((simulated_data[\"data\"],simulated_data[\"target\"].reshape(n_obs,1)),axis=1)) # don't print for too large a data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3624c",
   "metadata": {},
   "source": [
    "### Make a neural network, randomly initialize its weights, propagate input data\n",
    "\n",
    "Create a NN with 1 layer, 2 inputs, 1 output, thus 1 neuron in the layer. Propagate the data inputs through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cccdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_network_structure = [2,1]\n",
    "used_activation = leaky_relu\n",
    "weights = create_weights(used_network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim = len(weights_as_vector)\n",
    "print(\"weights=\",weights)\n",
    "print(\"nb. of NN parameters to learn, dim=\",dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = forward_propagation(inputs=simulated_data[\"data\"],weights=weights,activation_functions=used_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2afa19",
   "metadata": {},
   "source": [
    "Compare the data and the prediction of the network. Of course, at this point, no training is done so they are different. They just have the same format (provided a `reshape` is done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data vs prediction\\n\")\n",
    "print(np.append(simulated_data[\"target\"].reshape(-1,1),predicted_output,axis=1))\n",
    "plt.scatter(simulated_data[\"target\"], predicted_output, label='pred vs. data', color='blue')\n",
    "min_value = simulated_data[\"target\"].min()\n",
    "max_value = simulated_data[\"target\"].max()\n",
    "plt.plot([min_value, max_value], [min_value, max_value], color='red', linestyle='--', label='')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('prediction')\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea2bb7",
   "metadata": {},
   "source": [
    "## Error functions \n",
    "\n",
    "Utility functions to transform weights into a vector and vice versa. This is used in the calculation of the error function (the vector is transformed into NN weights, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with weight transformation functions\n",
    "weights = create_weights(used_network_structure)\n",
    "print('weights=\\n',weights)\n",
    "weights_as_vector, _ = weights_to_vector(weights)\n",
    "print('weights_as_vector=\\n',weights_as_vector)\n",
    "w2 = vector_to_weights(weights_as_vector, used_network_structure)\n",
    "print('weights back=\\n',w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c1487",
   "metadata": {},
   "source": [
    "We define 2 error functions, one for regression is the mean square error, the other is the cross-entropy error for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def cost_function_mse(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "    error = 0.5 * np.mean((y_predicted - y_observed)**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855acb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy\n",
    "def cost_function_entropy(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "    n = len(y_observed) \n",
    "    term_A = np.multiply(np.log(y_predicted),y_observed)\n",
    "    term_B = np.multiply(1-y_observed,np.log(1-y_predicted))  \n",
    "    error = - (1/n)*(np.sum(term_A)+np.sum(term_B))\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49026cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_with_parameters(vector_weights: np.ndarray,\n",
    "                          network_structure: List[int],\n",
    "                          activation_function: Callable,\n",
    "                          data: dict,\n",
    "                          cost_function: Callable,\n",
    "                          regularization: float = 0) -> float:\n",
    "    \n",
    "    weights = vector_to_weights(vector_weights,network_structure)\n",
    "    predicted_output = forward_propagation(data[\"data\"],weights,activation_function)\n",
    "    predicted_output = predicted_output.reshape(-1,)\n",
    "    \n",
    "    error = cost_function(predicted_output,data[\"target\"]) + regularization * np.sum(np.abs(vector_weights))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_network_structure and used_activation defined above\n",
    "used_data = simulated_data\n",
    "used_cost_function = cost_function_mse\n",
    "\n",
    "\n",
    "def neural_network_cost(vector_weights):\n",
    "    \n",
    "    cost = error_with_parameters(vector_weights,\n",
    "                                 network_structure = used_network_structure,\n",
    "                                 activation_function = used_activation,\n",
    "                                 data = used_data,\n",
    "                                 cost_function = used_cost_function)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0379da",
   "metadata": {},
   "source": [
    "Below, the cost function associated to the neural network is calculated from a simple vector in a manner similar to $f(x)$, therefore prone to optimization. The translation of the vector into as many weight matrices as necessary is done thanks to the \n",
    "* `used_network_structure`,\n",
    "* `used_activation`,\n",
    "* `used_data` and\n",
    "* `used_cost_function`\n",
    "defined above and passed implicitely thanks to Python's scoping rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call to the NN cost at a random point = random initialization of the weights and biases\n",
    "random_weights_as_vect = np.random.uniform(size=dim)\n",
    "neural_network_cost(random_weights_as_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddbb2a",
   "metadata": {},
   "source": [
    "### Learn the network by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443aa406",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = [-5] * 3 \n",
    "UB = [5] * 3\n",
    "printlevel = 1\n",
    "res = gradient_descent(func = neural_network_cost,\n",
    "                 start_x = np.array([0.28677805, -0.07982693,  0.37394315]),\n",
    "                 LB = LB, UB = UB,budget = 1000,printlevel=printlevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting\n",
    "# report optimization convergence\n",
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]), \n",
    "          LB=LB, UB=UB , printlevel=printlevel, logscale = False)\n",
    "# look at the network learned\n",
    "weights_best = vector_to_weights(res[\"x_best\"],used_network_structure) # extract weights of best network found\n",
    "print(\"Best NN weights:\\n\",weights_best)\n",
    "predicted_output = forward_propagation(used_data[\"data\"],weights_best,used_activation)\n",
    "plt.scatter(used_data[\"target\"], predicted_output, label='pred vs. data', color='blue')\n",
    "min_value = used_data[\"target\"].min()\n",
    "max_value = used_data[\"target\"].max()\n",
    "plt.plot([min_value, max_value], [min_value, max_value], color='red', linestyle='--', label='')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('prediction')\n",
    "plt.title('learned network')\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac9627",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Question 4: make your own network for regression</span>\n",
    "\n",
    "3. Generate 100 data points with the sphere function in 2 dimensions.\n",
    "4. Create a network with 2 inputs, 5 ReLU neurons in the hidden layer, and 1 output.\n",
    "5. Learn it on the quadratic data points you generated. Plot some results, discuss them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28abda",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Answer 4: make your own network for regression</span>\n",
    "\n",
    "Your code, your explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Génération de 100 points avec la fonction sphere en 2D\n",
    "\n",
    "used_function = sphere\n",
    "n_features = 2\n",
    "n_obs = 100\n",
    "LB_data = [-5] * n_features\n",
    "UB_data = [5] * n_features\n",
    "\n",
    "sphere_data = simulate_data_target(\n",
    "    fun=used_function,\n",
    "    n_features=n_features,\n",
    "    n_obs=n_obs,\n",
    "    LB=LB_data,\n",
    "    UB=UB_data\n",
    ")\n",
    "\n",
    "print(\"Échantillon des données générées (x1, x2, f(x)):\")\n",
    "print(np.concatenate(\n",
    "    (sphere_data[\"data\"][:5], sphere_data[\"target\"][:5].reshape(-1, 1)),\n",
    "    axis=1\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Création du réseau : 2 entrées -> 5 neurones ReLU -> 1 sortie\n",
    "\n",
    "network_structure = [2, 5, 1]\n",
    "activation_functions = [relu, linear]  # ReLU pour couche cachée, linear pour sortie\n",
    "\n",
    "weights = create_weights(network_structure)\n",
    "weights_vec, _ = weights_to_vector(weights)\n",
    "dim = len(weights_vec)\n",
    "\n",
    "print(f\"Architecture du réseau: {network_structure}\")\n",
    "print(f\"Nombre de paramètres à apprendre: {dim}\")\n",
    "print(f\"Poids initiaux (première couche):\\n{weights[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307820db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Définition de la fonction de coût pour le réseau\n",
    "\n",
    "used_network_structure = network_structure\n",
    "used_activation = activation_functions\n",
    "used_data = sphere_data\n",
    "used_cost_function = cost_function_mse\n",
    "\n",
    "def neural_network_cost(vector_weights):\n",
    "    cost = error_with_parameters(\n",
    "        vector_weights,\n",
    "        network_structure=used_network_structure,\n",
    "        activation_function=used_activation,\n",
    "        data=used_data,\n",
    "        cost_function=used_cost_function\n",
    "    )\n",
    "    return cost\n",
    "\n",
    "# Test avec poids aléatoires\n",
    "random_weights = np.random.uniform(size=dim)\n",
    "print(f\"Coût avec poids aléatoires: {neural_network_cost(random_weights):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4a. Apprentissage avec ReLU et pas fixe (baseline)\n",
    "\n",
    "LB = [-5] * dim\n",
    "UB = [5] * dim\n",
    "printlevel = 1\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPÉRIENCE 1: ReLU avec pas FIXE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "res_relu_fixed = gradient_descent_patched(\n",
    "    func=neural_network_cost,\n",
    "    start_x=np.random.uniform(low=LB, high=UB),\n",
    "    LB=LB, UB=UB,\n",
    "    budget=2000,\n",
    "    step_factor=1e-1,\n",
    "    direction_type=\"gradient\",\n",
    "    do_linesearch=False,\n",
    "    inertia=0.9,\n",
    "    printlevel=printlevel,\n",
    "    step_decay_type=\"none\"  # PAS DE DÉCROISSANCE\n",
    ")\n",
    "\n",
    "print(f\"\\nCoût final (ReLU, pas fixe): {res_relu_fixed['f_best']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4b. Apprentissage avec ReLU et pas décroissant\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPÉRIENCE 2: ReLU avec pas DÉCROISSANT (inverse_time)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "res_relu_decay = gradient_descent_patched(\n",
    "    func=neural_network_cost,\n",
    "    start_x=np.random.uniform(low=LB, high=UB),\n",
    "    LB=LB, UB=UB,\n",
    "    budget=2000,\n",
    "    step_factor=1e-1,\n",
    "    direction_type=\"gradient\",\n",
    "    do_linesearch=False,\n",
    "    inertia=0.9,\n",
    "    printlevel=printlevel,\n",
    "    step_decay_type=\"inverse_time\",  # DÉCROISSANCE INVERSE\n",
    "    step_decay_rate=5e-4\n",
    ")\n",
    "\n",
    "print(f\"\\nCoût final (ReLU, pas décroissant): {res_relu_decay['f_best']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5a. Test avec neurones SIGMOID (pas fixe)\n",
    "\n",
    "# Changer l'activation\n",
    "used_activation = [sigmoid, linear]  # Sigmoid pour couche cachée\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPÉRIENCE 3: SIGMOID avec pas FIXE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "res_sigmoid_fixed = gradient_descent_patched(\n",
    "    func=neural_network_cost,\n",
    "    start_x=np.random.uniform(low=LB, high=UB),\n",
    "    LB=LB, UB=UB,\n",
    "    budget=2000,\n",
    "    step_factor=1e-1,\n",
    "    direction_type=\"gradient\",\n",
    "    do_linesearch=False,\n",
    "    inertia=0.9,\n",
    "    printlevel=printlevel,\n",
    "    step_decay_type=\"none\"\n",
    ")\n",
    "\n",
    "print(f\"\\nCoût final (SIGMOID, pas fixe): {res_sigmoid_fixed['f_best']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cef127",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5b. Test avec neurones SIGMOID (pas décroissant)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPÉRIENCE 4: SIGMOID avec pas DÉCROISSANT (inverse_time)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "res_sigmoid_decay = gradient_descent_patched(\n",
    "    func=neural_network_cost,\n",
    "    start_x=np.random.uniform(low=LB, high=UB),\n",
    "    LB=LB, UB=UB,\n",
    "    budget=2000,\n",
    "    step_factor=5e-1,  # Pas initial plus grand pour sigmoid\n",
    "    direction_type=\"gradient\",\n",
    "    do_linesearch=False,\n",
    "    inertia=0.9,\n",
    "    printlevel=printlevel,\n",
    "    step_decay_type=\"inverse_time\",\n",
    "    step_decay_rate=5e-4  # Décroissance plus rapide\n",
    ")\n",
    "\n",
    "print(f\"\\nCoût final (SIGMOID, pas décroissant): {res_sigmoid_decay['f_best']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acf15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Comparaison des résultats\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ DES RÉSULTATS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_summary = {\n",
    "    \"ReLU + pas fixe\": res_relu_fixed['f_best'],\n",
    "    \"ReLU + pas décroissant\": res_relu_decay['f_best'],\n",
    "    \"Sigmoid + pas fixe\": res_sigmoid_fixed['f_best'],\n",
    "    \"Sigmoid + pas décroissant\": res_sigmoid_decay['f_best']\n",
    "}\n",
    "\n",
    "for method, cost in results_summary.items():\n",
    "    print(f\"{method:30s}: MSE = {cost:.6f}\")\n",
    "\n",
    "# Amélioration relative pour sigmoid\n",
    "improvement = (res_sigmoid_fixed['f_best'] - res_sigmoid_decay['f_best']) / res_sigmoid_fixed['f_best'] * 100\n",
    "print(f\"\\nAmélioration avec décroissance (sigmoid): {improvement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Visualisation de la convergence\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(res_relu_fixed['hist_f_best'], label='ReLU (pas fixe)', linewidth=2)\n",
    "plt.plot(res_relu_decay['hist_f_best'], label='ReLU (pas décroissant)', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Itération')\n",
    "plt.ylabel('MSE (log scale)')\n",
    "plt.yscale('log')\n",
    "plt.title('Convergence avec ReLU')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(res_sigmoid_fixed['hist_f_best'], label='Sigmoid (pas fixe)', linewidth=2, color='red')\n",
    "plt.plot(res_sigmoid_decay['hist_f_best'], label='Sigmoid (pas décroissant)', linewidth=2, linestyle='--', color='orange')\n",
    "plt.xlabel('Itération')\n",
    "plt.ylabel('MSE (log scale)')\n",
    "plt.yscale('log')\n",
    "plt.title('Convergence avec Sigmoid')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Visualisation des prédictions (meilleur résultat sigmoid)\n",
    "\n",
    "# Utiliser le meilleur réseau (sigmoid avec décroissance)\n",
    "used_activation = [sigmoid, linear]\n",
    "weights_best = vector_to_weights(res_sigmoid_decay[\"x_best\"], used_network_structure)\n",
    "predicted_output = forward_propagation(\n",
    "    used_data[\"data\"],\n",
    "    weights_best,\n",
    "    used_activation\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(used_data[\"target\"], predicted_output, alpha=0.6, color='blue', edgecolors='k')\n",
    "min_value = min(used_data[\"target\"].min(), predicted_output.min())\n",
    "max_value = max(used_data[\"target\"].max(), predicted_output.max())\n",
    "plt.plot([min_value, max_value], [min_value, max_value], color='red', linestyle='--', linewidth=2, label='y=x (parfait)')\n",
    "plt.xlabel('Valeurs réelles (sphere)', fontsize=12)\n",
    "plt.ylabel('Prédictions du réseau', fontsize=12)\n",
    "plt.title('Réseau 2-5-1 avec Sigmoid (pas décroissant)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coefficient de corrélation: {np.corrcoef(used_data['target'], predicted_output.flatten())[0,1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0f185",
   "metadata": {},
   "source": [
    "# **THE END**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398994f0-ebea-402d-b58d-8ee3d3ea108d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
