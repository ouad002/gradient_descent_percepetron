{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d42d9ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <center>UP3, Optimization for machine learning: regression with a Neural Network from scratch </center>\n",
    "\n",
    "This notebook contains the questions of the practical session along with complementary guidelines and examples. The code is written in Python. The questions are in red.\n",
    "\n",
    "First import all given code. You are encouraged to have a look at forward_propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d072762-fdbf-4fac-85cf-af3c9ba1fbc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:12:38.955517Z",
     "start_time": "2025-11-24T15:12:37.986072Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List\n",
    "\n",
    "from gradient_descent import gradient_descent\n",
    "from optim_utilities import print_rec\n",
    "from test_functions import (\n",
    "    linear_function,\n",
    "    ackley,\n",
    "    sphere,\n",
    "    quadratic,\n",
    "    rosen,\n",
    "    L1norm,\n",
    "    sphereL1,\n",
    "    rastrigin,\n",
    "    michalewicz,\n",
    "    schwefel\n",
    ")\n",
    "from restarted_gradient_descent import restarted_gradient_descent\n",
    "from random_search import random_opt  # always useful to compare optim algos to a random search\n",
    "\n",
    "# auto reload to reload functions imported that have been changed (cf. test_functions.sphereL1 for lbda)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52427bd6-0080-4d14-9820-df219bc26f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forward_propagation import (\n",
    "    forward_propagation, \n",
    "    create_weights, \n",
    "    vector_to_weights,\n",
    "    weights_to_vector)\n",
    "from activation_functions import (\n",
    "    relu,\n",
    "    sigmoid,\n",
    "    linear,\n",
    "    leaky_relu\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b875f8",
   "metadata": {},
   "source": [
    "### Data structure behind the forward propagation\n",
    "\n",
    "To start, let's have a look at the structure used to encode the network. Don't worry, more handy utility functions will be introduced soon to manipulate the network globally. \n",
    "\n",
    "The following network has 2 layers, the first going from 4 input components to the 3 internal neurons, the second going from the 3 internal neurons outputs to the 2 outputs. Don't forget the additional weight for the neurons biases.\n",
    "* `weight[l][i,j]` links entry `j` to output `i` in a layer `l`\n",
    "* the last column of the weights are the biases of the neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "inputs = np.array([[1,2,5,4],[1,0.2,0.15,0.024],[2,1.2,-1.01,-0.4]])\n",
    "# describe the neural net\n",
    "weights = [\n",
    "        np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1,-1],\n",
    "                [2,1,3,5,0],\n",
    "                [0.2,0.1,0.6,0.78,1]\n",
    "            ]\n",
    "        ),\n",
    "    np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1],\n",
    "                [2,1,3,5]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "activation = sigmoid\n",
    "# carry out forward propagation\n",
    "y=forward_propagation(inputs,weights,activation)\n",
    "# reporting\n",
    "print(f'This network has {inputs.shape[1]} inputs, {y.shape[1]} outputs and {inputs.shape[0]} data points')\n",
    "print('The predictions are:\\n',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb1fa2",
   "metadata": {},
   "source": [
    "### Create a data set \n",
    "The data set is made of points sampled randomly from a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829bf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_target(fun: Callable,\n",
    "                       n_features: int,\n",
    "                       n_obs: int,\n",
    "                       LB: List[float],\n",
    "                       UB: List[float]) -> dict:\n",
    "    \n",
    "    entry_data = np.random.uniform(low= LB,high=UB,\n",
    "                                   size=(n_obs, n_features))\n",
    "    target = np.apply_along_axis(fun, 1, entry_data)\n",
    "    \n",
    "    return {\"data\": entry_data, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34014047",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_function = linear_function\n",
    "n_features = 2\n",
    "n_obs = 10\n",
    "LB = [-5] * n_features\n",
    "UB = [5] * n_features\n",
    "simulated_data = simulate_data_target(fun = used_function,n_features = n_features,n_obs=n_obs,LB=LB,UB=UB)\n",
    "print('x,f(x) =\\n',np.concatenate((simulated_data[\"data\"],simulated_data[\"target\"].reshape(n_obs,1)),axis=1)) # don't print for too large a data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3624c",
   "metadata": {},
   "source": [
    "### Make a neural network, randomly initialize its weights, propagate input data\n",
    "\n",
    "Create a NN with 1 layer, 2 inputs, 1 output, thus 1 neuron in the layer. Propagate the data inputs through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cccdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_network_structure = [2,1]\n",
    "used_activation = leaky_relu\n",
    "weights = create_weights(used_network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim = len(weights_as_vector)\n",
    "print(\"weights=\",weights)\n",
    "print(\"nb. of NN parameters to learn, dim=\",dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = forward_propagation(inputs=simulated_data[\"data\"],weights=weights,activation_functions=used_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2afa19",
   "metadata": {},
   "source": [
    "Compare the data and the prediction of the network. Of course, at this point, no training is done so they are different. They just have the same format (provided a `reshape` is done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data vs prediction\\n\")\n",
    "print(np.append(simulated_data[\"target\"].reshape(-1,1),predicted_output,axis=1))\n",
    "plt.scatter(simulated_data[\"target\"], predicted_output, label='pred vs. data', color='blue')\n",
    "min_value = simulated_data[\"target\"].min()\n",
    "max_value = simulated_data[\"target\"].max()\n",
    "plt.plot([min_value, max_value], [min_value, max_value], color='red', linestyle='--', label='')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('prediction')\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea2bb7",
   "metadata": {},
   "source": [
    "## Error functions \n",
    "\n",
    "Utility functions to transform weights into a vector and vice versa. This is used in the calculation of the error function (the vector is transformed into NN weights, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with weight transformation functions\n",
    "weights = create_weights(used_network_structure)\n",
    "print('weights=\\n',weights)\n",
    "weights_as_vector, _ = weights_to_vector(weights)\n",
    "print('weights_as_vector=\\n',weights_as_vector)\n",
    "w2 = vector_to_weights(weights_as_vector, used_network_structure)\n",
    "print('weights back=\\n',w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c1487",
   "metadata": {},
   "source": [
    "We define 2 error functions, one for regression is the mean square error, the other is the cross-entropy error for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def cost_function_mse(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "    error = 0.5 * np.mean((y_predicted - y_observed)**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855acb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy\n",
    "def cost_function_entropy(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "    n = len(y_observed) \n",
    "    term_A = np.multiply(np.log(y_predicted),y_observed)\n",
    "    term_B = np.multiply(1-y_observed,np.log(1-y_predicted))  \n",
    "    error = - (1/n)*(np.sum(term_A)+np.sum(term_B))\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49026cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_with_parameters(vector_weights: np.ndarray,\n",
    "                          network_structure: List[int],\n",
    "                          activation_function: Callable,\n",
    "                          data: dict,\n",
    "                          cost_function: Callable,\n",
    "                          regularization: float = 0) -> float:\n",
    "    \n",
    "    weights = vector_to_weights(vector_weights,network_structure)\n",
    "    predicted_output = forward_propagation(data[\"data\"],weights,activation_function)\n",
    "    predicted_output = predicted_output.reshape(-1,)\n",
    "    \n",
    "    error = cost_function(predicted_output,data[\"target\"]) + regularization * np.sum(np.abs(vector_weights))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_network_structure and used_activation defined above\n",
    "used_data = simulated_data\n",
    "used_cost_function = cost_function_mse\n",
    "\n",
    "\n",
    "def neural_network_cost(vector_weights):\n",
    "    \n",
    "    cost = error_with_parameters(vector_weights,\n",
    "                                 network_structure = used_network_structure,\n",
    "                                 activation_function = used_activation,\n",
    "                                 data = used_data,\n",
    "                                 cost_function = used_cost_function)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0379da",
   "metadata": {},
   "source": [
    "Below, the cost function associated to the neural network is calculated from a simple vector in a manner similar to $f(x)$, therefore prone to optimization. The translation of the vector into as many weight matrices as necessary is done thanks to the \n",
    "* `used_network_structure`,\n",
    "* `used_activation`,\n",
    "* `used_data` and\n",
    "* `used_cost_function`\n",
    "defined above and passed implicitely thanks to Python's scoping rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call to the NN cost at a random point = random initialization of the weights and biases\n",
    "random_weights_as_vect = np.random.uniform(size=dim)\n",
    "neural_network_cost(random_weights_as_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddbb2a",
   "metadata": {},
   "source": [
    "### Learn the network by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443aa406",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = [-5] * 3 \n",
    "UB = [5] * 3\n",
    "printlevel = 1\n",
    "res = gradient_descent(func = neural_network_cost,\n",
    "                 start_x = np.array([0.28677805, -0.07982693,  0.37394315]),\n",
    "                 LB = LB, UB = UB,budget = 1000,printlevel=printlevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporting\n",
    "# report optimization convergence\n",
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]), \n",
    "          LB=LB, UB=UB , printlevel=printlevel, logscale = False)\n",
    "# look at the network learned\n",
    "weights_best = vector_to_weights(res[\"x_best\"],used_network_structure) # extract weights of best network found\n",
    "print(\"Best NN weights:\\n\",weights_best)\n",
    "predicted_output = forward_propagation(used_data[\"data\"],weights_best,used_activation)\n",
    "plt.scatter(used_data[\"target\"], predicted_output, label='pred vs. data', color='blue')\n",
    "min_value = used_data[\"target\"].min()\n",
    "max_value = used_data[\"target\"].max()\n",
    "plt.plot([min_value, max_value], [min_value, max_value], color='red', linestyle='--', label='')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('prediction')\n",
    "plt.title('learned network')\n",
    "plt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac9627",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Question 4: make your own network for regression</span>\n",
    "\n",
    "3. Generate 100 data points with the sphere function in 2 dimensions.\n",
    "4. Create a network with 2 inputs, 5 ReLU neurons in the hidden layer, and 1 output.\n",
    "5. Learn it on the quadratic data points you generated. Plot some results, discuss them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28abda",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Answer 4: make your own network for regression</span>\n",
    "\n",
    "Your code, your explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1efabb-f6ea-421e-9bd5-5a6a517df19c",
   "metadata": {},
   "source": [
    "## Correction to Question 4\n",
    "\n",
    "### Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79166f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_function = sphere\n",
    "n_features = 2\n",
    "n_obs = 100\n",
    "LBfeatures = [-5] * n_features\n",
    "UBfeatures = [5] * n_features\n",
    "simulated_data = simulate_data_target(fun = used_function,n_features = n_features,n_obs=n_obs,\n",
    "                                      LB=LBfeatures,UB=UBfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f76f3",
   "metadata": {},
   "source": [
    "### Create the network\n",
    "and calculate the cost function of the first, randomly initialized, network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d145dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_network_structure = [2,5,1]\n",
    "weights = create_weights(used_network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim = len(weights_as_vector) \n",
    "used_activation = leaky_relu\n",
    "# Other possibilities for describing activation functions: \n",
    "# as a single function copied into all of the neurons : [any of the possible activation function] e.g. [relu]\n",
    "# as a list whose size is the number of layers (one type of neurons per layer: [sigmoid,leaky_relu]\n",
    "# as a list of list where each neuron is described separately, ex with structure [2,5,1] \n",
    "#                           [[sigmoid,sigmoid,sigmoid,leaky_relu,leaky_relu],[leaky_relu]]\n",
    "used_data = simulated_data\n",
    "used_cost_function = cost_function_mse\n",
    "print(\"Number of weights to learn : \",dim)\n",
    "print(\"Initial cost of the NN : \",neural_network_cost(weights_as_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311fb300",
   "metadata": {},
   "source": [
    "### Learn the network\n",
    "Here we use a restarted version of the gradient descent because learning NN is **very** difficult when sigmoid activation functions are used. The reason is that sigmoids create flat plateaus and gradients tend to 0 up to numerical precision rapidly.\n",
    "A NN made of relu or leaky_relu neurons can be learned with a standard gradient-based method such as the ones provided in this course. \n",
    "With a `[2,5,1]` network, the following structures are examples that can be learned: \n",
    "```\n",
    "used_activation = [[sigmoid,sigmoid,sigmoid,leaky_relu,leaky_relu],[leaky_relu]]\n",
    "used_activation = [sigmoid,leaky_relu] \n",
    "used_activation = [leaky_relu] or [relu] \n",
    "```\n",
    "but `[sigmoid]` cannot be learned. For such structure, specialized optimizers such as ADAM are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = [-10] * dim\n",
    "UB = [10] * dim\n",
    "printlevel = 1\n",
    "res = restarted_gradient_descent(func=neural_network_cost, start_x=weights_as_vector,LB=LB,UB=UB,budget=50000,nb_restarts=5,\n",
    "                                 printlevel=printlevel)\n",
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]), \n",
    "          LB=LB, UB=UB , printlevel=printlevel, logscale = True)\n",
    "\n",
    "weights_best = vector_to_weights(res[\"x_best\"],used_network_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best NN weights:\",weights_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd42bc",
   "metadata": {},
   "source": [
    "Compare the network prediction to the true function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f334d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# remember the function used for generating data\n",
    "dimFunc = 2\n",
    "LBfunc = LBfeatures \n",
    "UBfunc = UBfeatures\n",
    "fun = used_function\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "x1 = np.linspace(start=LBfunc[0], stop=UBfunc[0],num=no_grid)\n",
    "x2 = np.linspace(start=LBfunc[1], stop=UBfunc[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "zNN = np.apply_along_axis(forward_propagation,0,xy,weights_best,used_activation)\n",
    "zNN = np.squeeze(zNN)\n",
    "\n",
    "# Create a figure with two 3D subplots side by side\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "# First 3D subplot\n",
    "ax1 = fig.add_subplot(121, projection='3d')  # 1 row, 2 columns, first subplot\n",
    "ax1.set_zlim(0,100)\n",
    "ax1.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "# ax1.plot_surface(x, y, z, cmap='viridis')\n",
    "ax1.set_title('target function')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f')\n",
    "# Second 3D subplot\n",
    "ax2 = fig.add_subplot(122, projection='3d')  # 1 row, 2 columns, second subplot\n",
    "ax2.set_zlim(0,100)\n",
    "ax2.plot_surface(x, y, zNN, cmap='jet', shade= \"false\")\n",
    "# ax2.plot_surface(x, y, zNN, cmap='plasma')\n",
    "ax2.set_title('NN prediction')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('f_{NN}')\n",
    "plt.tight_layout() # Adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0f185",
   "metadata": {},
   "source": [
    "# **THE END**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398994f0-ebea-402d-b58d-8ee3d3ea108d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
